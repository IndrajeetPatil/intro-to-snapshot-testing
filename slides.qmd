---
title: "Introduction to snapshot testing in R"
format:
  revealjs: 
    theme: simple
    slide-number: true
    preview-links: auto
    footer: "Source code for the slides can be found [here](https://github.com/IndrajeetPatil/intro-to-snapshot-testing)."
author: "Indrajeet Patil"
affiliation: "cynkra"
execute:
  echo: true
---

## Unit testing {.smaller}

The goal of a unit test is to capture the *expected* output of a function using *code* and making sure that *actual* output after any changes to the function matches the expected one.

::: incremental

- This insures against unintentionally changing function behaviour, or re-introducing already fixed bugs.

- It also makes sure that we will stay informed of any upstream dependency changes that might change the behaviour of our function.

- Tests also act as the most basic form of developer-focused documentation.

:::

## Unit testing with `{testthat}`: A recap {.smaller}

:::: {.columns}

::: {.column width='50%'}

::: {.callout-important}

## Test organization

Testing infrastructure for R package has the following hierarchy:

</br>

{{< fa regular file-code size=2xl >}} **Test file** : Tests for code situated in `R/foo.R` will typically be in the file `tests/testthat/test-foo.R`.

</br>

{{< fa solid flask size=2xl >}} **Tests** : A single file can contain multiple tests.

</br>

{{< fa solid equals size=2xl >}} **Expectations** : A single test can have multiple expectations that formalize what you expect code to do.

:::

:::

::: {.column width='50%'}

:::{.callout-tip}

## Example test file

- Every test is a call to `testthat::test_that()` function.

- Every expectation is represented by `testthat::expect_*()` function.

- You can generate a test file using `usethis::use_test()` function.

```{.r}
# File: tests/testthat/test-op.R

# test-1
test_that("multiplication works", {
  expect_equal(2 * 2, 4) # expectation-1
  expect_equal(-2 * 2, -4) # expectation-2
})

# test-2
test_that("addition works", {
  expect_equal(2 + 2, 4) # expectation-1
  expect_equal(-2 + 2, 0) # expectation-2
})

...
```

:::

:::

::::


## What is different about snapshot testing?

. . .

A **unit test** records the code to describe expected output.

</br>

<!-- Need to install the Quarto extension for fontawesome to work:
https://github.com/quarto-ext/fontawesome  -->

(actual) {{< fa regular file-code size=2xl >}} {{< fa solid arrows-left-right size=2xl >}} {{< fa solid file-code size=2xl >}} (expected)

</br>

. . .

A **snapshot test** records expected output in a separate, human-readable file.

</br>

(actual) {{< fa regular file-code size=2xl >}} {{< fa solid arrows-left-right size=2xl >}} {{< fa solid file-lines size=2xl >}} (expected)

## Why do we need snapshot testing?

If you develop R packages and have struggled to 

::: incremental

- test that text output *prints* as expected
- test that an entire file *is* as expected
- test that generated graphical output *looks* as expected
- update such tests *en masse*

::: 

. . .

then you should be excited to know more about *snapshot tests* (aka *golden tests*)! 🤩

# Prerequisites

Familiarity with writing unit tests using [`{testthat}`](https://testthat.r-lib.org/index.html).

If not, have a look at [this](https://r-pkgs.org/testing-basics.html) chapter from *R Packages* book.

## 

:::{.callout-important}

## *Nota bene*

In the following slides, in all snapshot tests, I include the following line of code:

```r
local_edition(3)
```

**You don't need to do this in your package tests!**

Just include this in the `DESCRIPTION` file to use `{testthat}` 3rd edition:

```r
Config/testthat/edition: 3
```

For more, see [this](https://testthat.r-lib.org/articles/third-edition.html) article.

:::

# Testing text outputs

Snapshot tests can be used to test that text output *prints* as expected.

To test functions that pretty-print R objects to the console, create elegant and informative warning/error messages, etc.

## Example function {.smaller}

Let's say we want to write a unit test for the following function:

:::: {.columns}

::: {.column width='60%'}

**Source code**

```{r}
print_movies <- function(keys, values) {
  paste0(
    "Movie: \n",
    paste0("  ", keys, ": ", values, collapse = "\n")
  )
}
```

:::

::: {.column width='40%'}

**Output**

```{r}
cat(print_movies(
  c("Title", "Director"),
  c("Salaam Bombay!", "Mira Nair")
))
```

:::

::::

. . .

</br>

Note that we want to test that the printed output *looks* as expected. 

Therefore, we need to check for all the little bells and whistles in the printed output.

## Example test {.smaller}

Even testing this simple function is a bit painful because we need to keep track of every escape character, every space, etc. 

```{r, echo=FALSE}
library(testthat)
```


```{r}
test_that("`print_movies()` prints as expected", {
  expect_equal(
    print_movies(
      c("Title", "Director"),
      c("Salaam Bombay!", "Mira Nair")
    ),
    "Movie: \n  Title: Salaam Bombay!\n  Director: Mira Nair"
  )
})
```

. . .

With a more complex code, it'd be impossible for a human to reason about what the output is supposed to look like.

. . .

:::{.callout-important}

If this is a utility function used by many other functions, changing its behaviour would entail *manually* changing expected outputs for many tests.

This is not maintainable! 😩

:::

## Alternative: Snapshot test {.smaller}

Instead, we can use `expect_snapshot()`, which, when run for the first time, generates a Markdown file with expected/reference output.

```{r include = FALSE}
snapper <- local_snapshotter()
snapper$start_file("slides.qmd", "test")
```

```{r}
test_that("`print_movies()` prints as expected", {
  local_edition(3)
  expect_snapshot(cat(print_movies(
    c("Title", "Director"),
    c("Salaam Bombay!", "Mira Nair")
  )))
})
```

```{r, include = FALSE}
# Reset snapshot test
snapper$end_file()
snapper$start_file("slides.qmd", "test")
```

. . .

:::{.callout-warning}

The first time a snapshot is created, it becomes *the truth* against which future function behaviour will be compared. 

Thus, it is **crucial** that we carefully check that the output is indeed as expected. 🔎 

:::

## Human-readable Markdown file {.smaller}

Compared to our unit test code representing the expected output

```r
"Movie: \n  Title: Salaam Bombay!\n  Director: Mira Nair"
```

notice how much more human-friendly the Markdown output is!

```md
Code
  cat(print_movies(c("Title", "Director"), c("Salaam Bombay!", "Mira Nair")))
Output
  Movie: 
    Title: Salaam Bombay!
    Director: Mira Nair
```

It is easy to *see* what the printed text output is *supposed* to look like. In other words, snapshot tests are useful when the *intent* of the code can only be verified by a human.

. . . 

:::{.callout-note}

## More about snapshot Markdown files

- If test file is called `test-foo.R`, the snapshot will be saved to `test/testthat/_snaps/foo.md`.

- If there are multiple snapshot tests in a single file, corresponding snapshots will also share the same `.md` file.

- By default, `expect_snapshot()` will capture the code, the object values, and any side-effects.

:::

## What test success looks like {.smaller}

If you run the test again, it'll succeed:

```{r}
test_that("`print_movies()` prints as expected", {
  local_edition(3)
  expect_snapshot(cat(print_movies(
    c("Title", "Director"),
    c("Salaam Bombay!", "Mira Nair")
  )))
})
```

```{r, include = FALSE}
# Reset snapshot test
snapper$end_file()
snapper$start_file("slides.qmd", "test")
```

. . .

</br>

:::{.callout-note}

### Why does my test fail on a re-run?

If testing a snapshot you just generated fails on re-running the test, this is most likely because your test is not deterministic. For example, if your function deals with random number generation.

In such cases, setting a seed (e.g. `set.seed(42)`) should help.

:::

## What test failure looks like {.smaller}

When function changes, snapshot doesn't match the reference, and the test fails:

:::: {.columns}

::: {.column width='30%'}

**Changes to function**

```{.r code-line-numbers="5"}
print_movies <- function(keys, values) {
  paste0(
    "Movie: \n",
    paste0(
      "  ", keys, "- ", values,
      collapse = "\n"
    )
  )
}
```

```{r}
#| echo: false
print_movies <- function(keys, values) {
  paste0(
    "Movie: \n",
    paste0("  ", keys, "- ", values, collapse = "\n")
  )
}
```

</br>

The test failure message provides a diff between what was expected (`-` rows) vs observed (`+` rows).

```r
- "    Title: Salaam Bombay!"
+ "    Title- Salaam Bombay!"
- "    Director: Mira Nair"
+ "    Director- Mira Nair"
```

:::

::: {.column width='70%'}

**Test failure**

```{.r}
test_that("`print_movies()` prints as expected", {
  expect_snapshot(cat(print_movies(
    c("Title", "Director"),
    c("Salaam Bombay!", "Mira Nair")
  )))
})
```

```{r, echo=FALSE, error=TRUE}
test_that("`print_movies()` prints as expected", {
  local_edition(3)
  expect_snapshot(cat(print_movies(
    c("Title", "Director"),
    c("Salaam Bombay!", "Mira Nair")
  )))
})
```

:::

::::


## Fixing tests {.smaller}

Message accompanying failed tests make it explicit how to fix them.

. . . 

- If the change was *deliberate*, you can accept the new snapshot as the current *truth*.

```r
* Run `snapshot_accept('foo.md')` to accept the change
```

- If this was *unexpected*, you can review the changes, and decide whether to change the snapshot or to correct the function behaviour instead.

```r
* Run `snapshot_review('foo.md')` to interactively review the change
```

. . . 

</br>

:::{.callout-tip}

## Fixing multiple snapshot tests

If this is a utility function used by many other functions, changing its behaviour would lead to failure of many tests. 

You can update *all* new snapshots with `snapshot_accept()`. And, of course, check the diffs to make sure that the changes are expected.

:::

## Capturing messages and warnings {.smaller}

So far we have tested text output printed to the console, but we can also use snapshots to capture messages, warnings, and errors.

:::: {.columns}

::: {.column width='50%'}

**message**

```{r}
f <- function() message("Some info for you.")

test_that("f() messages", {
  local_edition(3)
  expect_snapshot(f())
})
```

:::

::: {.column width='50%'}

**warning**

```{r}
g <- function() warning("Managed to recover.")

test_that("g() warns", {
  local_edition(3)
  expect_snapshot(g())
})
```

:::

::::

:::{.callout-tip}

Snapshot records both the *condition* and the corresponding *message*.

We can now rest assured that the users are getting informed the way we want! 😌

:::

## Capturing errors {.smaller}

In case of an error, the function `expect_snapshot()` itself will produce an error. 
You have two ways around this:

:::: {.columns}

::: {.column width='40%'}

**Option-1**

```{.r code-line-numbers="3"}
test_that("`log()` errors", {
  local_edition(3)
  expect_snapshot(log("x"), error = TRUE)
})
```

```{r, echo=FALSE}
test_that("`log()` errors", {
  local_edition(3)
  expect_snapshot(log("x"), error = TRUE)
})
```

:::

::: {.column width='60%'}

**Option-2**

```{.r code-line-numbers="3"}
test_that("`log()` errors", {
  local_edition(3)
  expect_snapshot_error(log("x"))
})
```

```{r, echo=FALSE}
test_that("`log()` errors", {
  local_edition(3)
  expect_snapshot_error(log("x"))
})
```

:::

::::

:::{.callout-tip}

### Which option should I use?

- If you want to capture both the code and the error message, use `expect_snapshot(..., error = TRUE)`.

- If you want to capture only the error message, use `expect_snapshot_error()`.

:::

## Further reading {.smaller}

- `{testthat}` article on [snapshot testing](https://testthat.r-lib.org/articles/snapshotting.html)

- Introduction to [golden testing](https://ro-che.info/articles/2017-12-04-golden-tests)

- Docs for [Jest](https://jestjs.io/docs/snapshot-testing) library in JavaScript, which inspired snapshot testing implementation in `{testthat}`


# Testing graphical outputs

To create graphical expectations, we will use `{testthat}` extension package: [`{vdiffr}`](https://vdiffr.r-lib.org/).

## How does `{vdiffr}` work? {.smaller}

`{vdiffr}` introduces `expect_doppelganger()` to generate `{testthat}` expectations for graphics. It does this by writing SVG snapshot files for outputs!

. . . 

The figure to test can be:

- a `ggplot` object (from `ggplot2::ggplot()`)
- a `recordedplot` object (from `grDevices::recordPlot()`)
- any object with a `print()` method

. . . 

:::{.callout-note}

- If test file is called `test-foo.R`, the snapshot will be saved to `test/testthat/_snaps/foo` folder.

- In this folder, there will be one `.svg` file for every test in `test-foo.R`.

- The name for the `.svg` file will be sanitized version of `title` argument to `expect_doppelganger()`.

:::

## Example function {.smaller}

Let's say we want to write a unit test for the following function:

:::: {.columns}

::: {.column width='50%'}

**Source code**

```{r}
library(ggplot2)

create_scatter <- function() {
  ggplot(mtcars, aes(wt, mpg)) +
    geom_point(size = 3, alpha = 0.75) +
    geom_smooth(method = "lm")
}
```
:::

::: {.column width='50%'}

**Output**

```{r}
#| out.width: "100%"
create_scatter()
```

:::

::::

. . .

Note that we want to test that the graphical output *looks* as expected, and this expectation is difficult to capture with a unit test.

## Graphical snapshot test {.smaller}

We can use `expect_doppelganger()` from `{vdiffr}` to test this!

</br>

. . .

The *first time* you run the test, it'd generate an `.svg` file with expected output.

```{r include = FALSE}
library(vdiffr)

snapper <- local_snapshotter()
snapper$start_file("slides.qmd", "test")
```

```{r}
test_that("`create_scatter()` plots as expected", {
  local_edition(3)
  expect_doppelganger(
    title = "create scatter",
    fig = create_scatter(),
  )
})
```

```{r, include = FALSE}
# Reset snapshot test
snapper$end_file()
snapper$start_file("slides.qmd", "test")
```

. . .

:::{.callout-warning}

The first time a snapshot is created, it becomes *the truth* against which future function behaviour will be compared. 

Thus, it is **crucial** that we carefully check that the output is indeed as expected. 🔎 

We can open `.svg` snapshot files in a web browser for closer inspection.

:::

## What test success looks like {.smaller}

If we run the test again, it'll succeed:

```{r}
test_that("`create_scatter()` plots as expected", {
  local_edition(3)
  expect_doppelganger(
    title = "create scatter",
    fig = create_scatter(),
  )
})
```

```{r, include = FALSE}
# Reset snapshot test
snapper$end_file()
snapper$start_file("slides.qmd", "test")
```

## What test failure looks like {.smaller}

When function changes, snapshot doesn't match the reference, and the test fails:

:::: {.columns}

::: {.column width='40%'}

**Changes to function**

```{.r code-line-numbers="3"}
create_scatter <- function() {
  ggplot(mtcars, aes(wt, mpg)) +
    geom_point(size = 2, alpha = 0.85) +
    geom_smooth(method = "lm")
}
```

```{r}
#| echo: false
create_scatter <- function() {
  ggplot(mtcars, aes(wt, mpg)) +
    geom_point(size = 2, alpha = 0.85) +
    geom_smooth(method = "lm")
}
```

:::

::: {.column width='60%'}

**Test failure**

```{r, error=TRUE}
test_that("`create_scatter()` plots as expected", {
  local_edition(3)
  expect_doppelganger(
    title = "create scatter",
    fig = create_scatter(),
  )
})
```

:::

::::

## Fixing tests {.smaller}

Running `snapshot_review()` launches a Shiny app which can be used to either accept or reject the new output(s).

```{r}
#| echo: false
#| out.width: "60%"
knitr::include_graphics("media/shiny_app_graphics.mov")
```

##

:::{.callout-tip}

## Why are my snapshots for plots failing?! 😔

If tests fail even if the function didn't change, it can be due to any of the following reasons:

- R's graphics engine changed
- `{ggplot2}` itself changed
- non-deterministic behaviour
- changes in system libraries

For these reasons, snapshot tests for plots tend to be fragile and are not run on CRAN machines by default.

:::

## Further reading

- `{vdiffr}` package [website](https://vdiffr.r-lib.org/)

# Testing entire files

Whole file snapshot testing makes sure that media, data frames, text files, etc. are as expected.

## Writing test {.smaller}

Let's say we want to test JSON files generated by `jsonlite::write_json()`.

:::: {.columns}

::: {.column width='50%'}

**Test**

```{r include = FALSE}
snapper <- local_snapshotter()
snapper$start_file("slides.qmd", "test")
```

```{r}
# File: tests/testthat/test-write-json.R
test_that("json writer works", {
  local_edition(3)
  
  r_to_json <- function(x) {
    path <- tempfile(fileext = ".json")
    jsonlite::write_json(x, path)
    path
  }

  x <- list(1, list("x" = "a"))
  expect_snapshot_file(r_to_json(x), "demo.json")
})
```

```{r, include = FALSE}
# Reset snapshot test
snapper$end_file()
snapper$start_file("slides.qmd", "test")
```

**Snapshot**

```json
[[1],{"x":["a"]}]
```

:::

::: {.column width='50%'}

:::{.callout-note}

- To snapshot a file, we need to write a helper function that provides its path.

- If a test file is called `test-foo.R`, the snapshot will be saved to `test/testthat/_snaps/foo` folder.

- In this folder, there will be one file (e.g. `.json`) for every `expect_snapshot_file()` expectation in `test-foo.R`.

- The name for snapshot file is taken from `name` argument to `expect_snapshot_file()`.

:::

:::

::::


## What test success looks like {.smaller}

If we run the test again, it'll succeed:

```{r}
# File: tests/testthat/test-write-json.R
test_that("json writer works", {
  local_edition(3)
  
  r_to_json <- function(x) {
    path <- tempfile(fileext = ".json")
    jsonlite::write_json(x, path)
    path
  }

  x <- list(1, list("x" = "a"))
  expect_snapshot_file(r_to_json(x), "demo.json")
})
```

```{r, include = FALSE}
# Reset snapshot test
snapper$end_file()
snapper$start_file("slides.qmd", "test")
```

## What test failure looks like {.smaller}

If the new output doesn't match the expected one, the test will fail:

```{.r code-line-numbers="11"}
# File: tests/testthat/test-write-json.R
test_that("json writer works", {
  local_edition(3)
  
  r_to_json <- function(x) {
    path <- tempfile(fileext = ".json")
    jsonlite::write_json(x, path)
    path
  }

  x <- list(1, list("x" = "b"))
  expect_snapshot_file(r_to_json(x), "demo.json")
})
```

```{r, error=TRUE}
#| echo: false
# File: tests/testthat/test-write-json.R
test_that("json writer works", {
  local_edition(3)
  
  r_to_json <- function(x) {
    path <- tempfile(fileext = ".json")
    jsonlite::write_json(x, path)
    path
  }

  x <- list(1, list("x" = "b"))
  expect_snapshot_file(r_to_json(x), "demo.json")
})
```

## Fixing tests {.smaller}

Running `snapshot_review()` launches a Shiny app which can be used to either accept or reject the new output(s).

```{r}
#| echo: false
knitr::include_graphics("media/json_snapshot.png")
```

## Further reading {.smaller}

Documentation for [`expect_snapshot_file()`](https://testthat.r-lib.org/reference/expect_snapshot_file.html)

# Testing Shiny applications

To write formal tests for Shiny applications, we will use `{testthat}` extension package: [`{shinytest2}`](https://rstudio.github.io/shinytest2/).

## How does `{shinytest2}` work? {.smaller}

`{shinytest2}` uses a Shiny app (how meta! 😅) to record user interactions with the app and generate snapshots of the application's state. Future behaviour of the app will be compared against these snapshots to check for any changes.

. . . 

Exactly how tests for Shiny apps in R package are written depends on how the app is stored.
There are two possibilities, and we will discuss them both separately.

. . . 

</br>

:::: {.columns}

::: {.column width='50%'}

**Stored in `/inst` folder**

```
├── DESCRIPTION
├── R
├── inst
│   └── sample_app
│       └── app.R
```

:::

::: {.column width='50%'}

**Returned by a function**

```
├── DESCRIPTION
├── R
│   └── app-function.R
```

:::

::::

<!-- Don't add an indefinite article; it breaks the phrase into two lines -->

# Shiny app in subdirectory 

</br>

```
├── DESCRIPTION
├── R
├── inst
│   └── sample_app
│       └── app.R
```

## Example app {.smaller}

Let's say this app resides in the `inst/unitConverter/app.R` file.

<!-- 
  TODO: How to embed a Shiny app? 
  If I just include the code in R chunk, I see:
  "Shiny applications not supported in static R Markdown documents"
-->

:::: {.columns}

::: {.column width='40%'}

**App**

```{r}
#| echo: false
knitr::include_graphics("media/shiny_app.mov")
```

:::

::: {.column width='60%'}

**Code**

```{.r}
library(shiny)

ui <- fluidPage(
  titlePanel("Convert kilograms to grams"),
  numericInput("kg", "Weight (in kg)", value = 0),
  textOutput("g")
)

server <- function(input, output, session) {
  output$g <- renderText(
    paste0("Weight (in g): ", input$kg * 1000)
  )
}

shinyApp(ui, server)
```

:::

::::

## Generating a test {.smaller}

To create a snapshot test, go to the app directory and run `record_test()`.

```{r}
#| echo: false
#| out.width: "60%"
knitr::include_graphics("media/shinytest_record.mov")
```

## Auto-generated artifacts {.smaller}

:::: {.columns}

::: {.column width='60%'}

**Test**

```{.r}
library(shinytest2)

test_that("{shinytest2} recording: unitConverter", {
  app <- AppDriver$new(
  name = "unitConverter", height = 543, width = 426)
  app$set_inputs(kg = 1)
  app$set_inputs(kg = 10)
  app$expect_values()
})

```

:::

::: {.column width='40%'}

**Snapshot**

```{r}
#| echo: false
#| out-width: "100%"
knitr::include_graphics("media/unitConverter-001_.png")
```

:::

::::

:::{.callout-note}

- `record_test()` will auto-generate a test file in the app directory. The test script will be saved in a *subdirectory* of the app (`inst/my-app/tests/testthat/test-shinytest2.R`).

- There will be one `/tests` folder inside *every* app folder.

- The snapshots are saved as `.png` file in `tests/testthat/test-shinytest2/_snaps/{.variant}/shinytest2`. The `{.variant}` here corresponds to operating system and R version used to record tests. For example, `_snaps/windows-4.1/shinytest2`.

:::

## Creating a driver script {.smaller}

Note that currently our test scripts and results are in the `/inst` folder, but we'd also want to run these tests automatically using `{testthat}`.

For this, we will need to write a driver script like the following:

```{.r}
library(shinytest2)

test_that("`unitConverter` app works", {
  appdir <- system.file(package = "package_name", "unitConverter")
  test_app(appdir)
})
```

Now the Shiny apps will be tested with the rest of the source code in the package! 🎊	

:::{.callout-tip}

We save the driver test in the `/tests` folder (`tests/testthat/test-inst-apps.R`), alongside other tests.

:::

## What test failure looks like {.smaller}

Let's say, while updating the app, we make a mistake, which leads to a failed test.

**Changed code with mistake**

```{.r code-line-numbers="9"}
ui <- fluidPage(
  titlePanel("Convert kilograms to grams"),
  numericInput("kg", "Weight (in kg)", value = 0),
  textOutput("g")
)

server <- function(input, output, session) {
  output$g <- renderText(
    paste0("Weight (in kg): ", input$kg * 1000) # should be `"Weight (in g): "`
  )
}

shinyApp(ui, server)
```

**Test failure JSON diff**

```{.json code-line-number="6"}
Diff in snapshot file `shinytest2unitConverter-001.json`
< before                            > after                           
@@ 4,5 @@                           @@ 4,5 @@                         
    },                                  },                            
    "output": {                         "output": {                   
<     "g": "Weight (in g): 10000"   >     "g": "Weight (in kg): 10000"
    },                                  },                            
    "export": {                         "export": {    
```


## Updating snapshots {.smaller}

Fixing this test will be similar to fixing any other snapshot test we've seen thus far.

`{testthat2}` provides a Shiny app for comparing the old and new snapshots.

```{r}
#| echo: false
#| out.width: "60%"
knitr::include_graphics("media/shinytest_failure.mov")
```

# Function returns Shiny app

</br>

```
├── DESCRIPTION
├── R
│   └── app-function.R
```

## Example app and test {.smaller}

The only difference in testing workflow when Shiny app objects are created by functions is that we will write the test ourselves, instead of `{shinytest2}` auto-generating it.

:::: {.columns}

::: {.column width='55%'}

**Source code**

```{.r}
# File: R/unit-converter.R
unitConverter <- function() {
  ui <- fluidPage(
    titlePanel("Convert kilograms to grams"),
    numericInput("kg", "Weight (in kg)", value = 0),
    textOutput("g")
  )

  server <- function(input, output, session) {
    output$g <- renderText(
      paste0("Weight (in g): ", input$kg * 1000)
    )
  }

  shinyApp(ui, server)
}
```


:::

::: {.column width='45%'}

**Test file to modify**

```{.r}
# File: tests/testthat/test-unit-converter.R
test_that("unitConverter app works", {
  shiny_app <- unitConverter()
  app <- AppDriver$new(shiny_app)
})
```


:::

::::

## Generating test and snapshots {.smaller}

We call `record_test()` directly on a Shiny app object, copy-paste commands to the test script, and run `devtools::test_active_file()` to generate snapshots.

```{r}
#| echo: false
#| out.width: "60%"
knitr::include_graphics("media/shiny_app_function_return.mov")
```

## Testing apps from frameworks {.smaller}

This testing workflow is also relevant for app frameworks (e.g. [`{golem}`](https://thinkr-open.github.io/golem/index.html), [`{rhino}`](https://appsilon.github.io/rhino/), etc.).

:::: {.columns}

::: {.column width='50%'}

**`{golem}`**

Function in `run_app.R`
returns app.

```
├── DESCRIPTION 
├── NAMESPACE 
├── R 
│   ├── app_config.R 
│   ├── app_server.R 
│   ├── app_ui.R 
│   └── run_app.R 
```

:::

::: {.column width='50%'}

**`{rhino}`**

Function in `app.R`
returns app.

```
├── app
│   ├── js
│   │   └── index.js
│   ├── logic
│   │   └── __init__.R
│   ├── static
│   │   └── favicon.ico
│   ├── styles
│   │   └── main.scss
│   ├── view
│   │   └── __init__.R
│   └── main.R
├── tests
│   ├── ...
├── app.R
├── RhinoApplication.Rproj
├── dependencies.R
├── renv.lock
└── rhino.yml
```

:::

::::


## Final directory structure {.smaller}

The final location of the tests and snapshots should look like the following for the two possible ways Shiny apps are included in R packages.

</br>

:::: {.columns}

::: {.column width='50%'}

**Stored in `/inst` folder**

```
├── DESCRIPTION
├── R
├── inst
│   └── sample_app
│       ├── app.R
│       └── tests
│           ├── testthat
│           │   ├── _snaps
│           │   │   └── shinytest2
│           │   │       └── 001.json
│           │   └── test-shinytest2.R
│           └── testthat.R
└── tests
    ├── testthat
    │   └── test-inst-apps.R
    └── testthat.R
```

:::

::: {.column width='50%'}

**Returned by a function**

```
├── DESCRIPTION
├── R
│   └── app-function.R
└── tests
    ├── testthat
    │   ├── _snaps
    │   │   └── app-function
    │   │       └── 001.json
    │   └── test-app-function.R
    └── testthat.R
```

:::

::::

## Testing multiple apps {.smaller}

For the sake of completeness, here is what the test directory structure would like when there are multiple apps in a single package.

:::: {.columns}

::: {.column width='50%'}

**Stored in `/inst` folder**

```
├── DESCRIPTION
├── R
├── inst
│   └── sample_app1
│       ├── app.R
│       └── tests
│           ├── testthat
│           │   ├── _snaps
│           │   │   └── shinytest2
│           │   │       └── 001.json
│           │   └── test-shinytest2.R
│           └── testthat.R
│   └── sample_app2
│       ├── app.R
│       └── tests
│           ├── testthat
│           │   ├── _snaps
│           │   │   └── shinytest2
│           │   │       └── 001.json
│           │   └── test-shinytest2.R
│           └── testthat.R
└── tests
    ├── testthat
    │   └── test-inst-apps.R
    └── testthat.R
```

:::

::: {.column width='50%'}

**Returned by a function**

```
├── DESCRIPTION
├── R
│   └── app-function1.R
│   └── app-function2.R
└── tests
    ├── testthat
    │   ├── _snaps
    │   │   └── app-function1
    │   │       └── 001.json
    │   │   └── app-function2
    │   │       └── 001.json
    │   └── test-app-function1.R
    │   └── test-app-function2.R
    └── testthat.R
```

:::

::::

## Advanced topics {.smaller}

The following are some advanced topics that are beyond the scope of the current presentation, but you may wish to know more about.

:::{.callout-tip}

## Extra

- If you want to test Shiny apps with continuous integration using `{shinytest2}`, read [this](https://rstudio.github.io/shinytest2/articles/use-ci.html) article.

- `{shinytest2}` is a successor to `{shinytest}` package. If you want to migrate from the latter to the former, have a look at [this](https://rstudio.github.io/shinytest2/articles/z-migration.html).

:::



## Further reading {.smaller}

- [Testing](https://mastering-shiny.org/scaling-testing.html) chapter from *Mastering Shiny* book

- `{shinytest2}` article introducing its [workflow](https://rstudio.github.io/shinytest2/articles/shinytest2.html)

- `{shinytest2}` article on how to test apps in [R packages](https://rstudio.github.io/shinytest2/articles/use-package.html)

# Headaches 

It's not all kittens and roses when it comes to snapshot testing. 

Let's now see some issues we might run into while using them. 🤕

## Failures in non-interactive environments {.smaller}

If snapshots fail locally, we can just run `snapshot_review()`, but what if they fail in non-interactive environments (on CI/CD platforms, during `R CMD Check`, etc.)?

The easiest solution is to copy the new snapshots to the local folder and run `snapshot_review()`.

:::{.callout-tip}

If expected snapshot is called (e.g.) `foo.svg`, there will be a new snapshot file `foo.new.svg` in the same folder when the test fails.

`snapshot_review()` compares these files to reveal how the outputs have changed.

:::

But where can we find the new snapshots?

## Accessing new snapshots {.smaller}

In local `R CMD Check`, we can find new snapshots in `.Rcheck` folder:

```r
package_name.Rcheck/tests/testthat/_snaps/
```

On CI/CD platforms, we can find snapshots in artifacts folder:

- [AppVeyor](https://www.appveyor.com/docs/packaging-artifacts/)

```{r, echo=FALSE, fig.height=1.5}
knitr::include_graphics("media/AppVeyor.png")
```

- [GitHub actions](https://github.com/r-lib/actions/tree/v2-branch/check-r-package)

```yaml
      - uses: r-lib/actions/check-r-package@v2
        with:
          upload-snapshots: true
```

## Code review with snapshot tests {.smaller}

Despite snapshot tests making the expected outputs more human-readable, given a big enough change and complex enough output, sometimes it can be challenging to review changes to snapshots. 

How do we review pull requests with complex snapshots changes? 

## {.smaller}

:::panel-tabset

### Option-1

Use tools provided for code review by hosting platforms (like GitHub).
For example, to review changes in SVG snapshots:

```{r}
#| echo: false
#| out.width: "70%"
knitr::include_graphics("media/github_pr.mov")
```

### Option-2

Locally open the PR branch and play around with the changes to see if the new behaviour makes sense. 🤷 

:::

## Danger of silent failures {.smaller}

Given their fragile nature, snapshot tests are skipped on CRAN by default. 

Although this makes sense, it means that we miss out on anything but a breaking change from upstream dependency. E.g., if `{ggplot2}` (hypothetically) changes how the points look, we won't know about this change until we *happen to* run our snapshot tests again locally or on CI/CD.

Unit tests run on CRAN, on the other hand, will fail and we will be immediately informed about it.

:::{.callout-tip collapse=false appearance='default' icon=true}

A way to insure against such silent failures is to run tests **daily** on CI/CD platforms (e.g. AppVeyor nightly builds).

:::

# Parting wisdom

What *not* to do

## Don't use snapshot tests for *everything* {.smaller}

It is tempting to use them everywhere out of laziness. But they are sometimes inappropriate (e.g. when testing requires external benchmarking).

Let's say we write a function to extract estimates from a regression model.

```{.r}
extract_est <- function(m) m$coefficients
```

Its test should compare results against an external benchmark, and not a snapshot.

:::: {.columns}

::: {.column width='50%'}

**Good**

```{.r}
test_that("`extract_est()` works", {
  m <- lm(wt ~ mpg, mtcars)
  expect_equal(
    extract_est(m)[[1]],
    m$coefficients[[1]]
  )
})
```


:::

::: {.column width='50%'}

**Bad**

```{.r}
test_that("`extract_est()` works", {
  m <- lm(wt ~ mpg, mtcars)
  expect_snapshot(extract_est(m))
})
```

:::

::::

## Snapshot for humans, not machines {.smaller}

Snapshot testing is appropriate when the human needs to be in the loop to make sure that things are working as expected. Therefore, the snapshots should be human readable.

E.g. if we write a function that plots something:

```{.r}
point_plotter <- function() ggplot(mtcars, aes(wt, mpg)) + geom_point()
```

To test it, we should snapshot the *plot*, and not the underlying *data*, which is hard to make sense of for a human.

:::: {.columns}

::: {.column width='50%'}

**Good**

```{.r}
test_that("`point_plotter()` works", {
  expect_doppelganger(
    "point-plotter-mtcars",
    point_plotter()
  )
})
```

:::

::: {.column width='50%'}

**Bad**

```{.r}
test_that("`point_plotter()` works", {
  p <- point_plotter()
  pb <- ggplot_build(p)
  expect_snapshot(pb$data)
})
```

:::

::::

## Don't **blindly accept** snapshot changes {.smaller}

Resist formation of such a habit.

`{testthat}` provides tools to make it very easy to review changes, so no excuses! 


# Self-study 

In this presentation, we deliberately kept the examples and the tests simple. 

To see a more realistic usage of snapshot tests, you can study open-source test suites.

## Suggested repositories {.smaller}

::: columns

<!-- 1st column -->
::: {.column width="33%"}
### Print outputs

- [`{cli}`](https://github.com/r-lib/cli/tree/main/tests/testthat) (for testing command line interfaces)

- [`{pkgdown}`](https://github.com/r-lib/pkgdown/tree/main/tests/testthat) (for testing generated HTML documents)

- [`{dbplyr}`](https://github.com/tidyverse/dbplyr/tree/main/tests/testthat) (for testing printing of generated SQL queries)

- [`{gt}`](https://github.com/rstudio/gt/tree/master/tests/testthat) (for testing table printing)
:::

<!-- 2nd column -->
::: {.column width="33%"}
### Visualizations

- [`{ggplot2}`](https://github.com/tidyverse/ggplot2/tree/main/tests/testthat)

- [`{ggstatsplot}`](https://github.com/IndrajeetPatil/ggstatsplot/tree/main/tests/testthat)
:::

<!-- 3rd column -->
::: {.column width="33%"}
### Shiny apps 

- [`{shinytest2}`](https://github.com/rstudio/shinytest2/tree/main/tests/testthat)

- [`{designer}`](https://github.com/ashbaldry/designer/tree/dev/tests/testthat)

:::

:::

# Activities

If you feel confident enough to contribute to open-source projects to practice these skills, here are some options.

## Practice makes it perfect {.smaller}

These are only suggestions. Feel free to contribute to any project you like! 🤝

:::{.callout-tip}

## Suggestions 

- See if `{ggplot2}` [extensions](https://exts.ggplot2.tidyverse.org/gallery/) you like use snapshot tests for graphics. If not, you can add them for key functions.

- Check out hard reverse dependencies of [`{shiny}`](https://cran.r-project.org/web/packages/shiny/index.html), and add snapshot tests using `{shinytest2}` to an app of your liking.

- Add more `{vdiffr}` snapshot tests to plotting functions in [`{see}`](https://github.com/easystats/see), a library for statistical visualizations (I can chaperone your PRs here).

- `{shinytest2}` is the successor to `{shinytest}` package. Check out which packages currently use it for [testing](https://cran.r-project.org/web/packages/shinytest/index.html) Shiny apps, and see if you can use `{shinytest2}` instead (see how-to [here](https://rstudio.github.io/shinytest2/articles/z-migration.html)).

:::

## General reading {.smaller}

Although current presentation is focused only on snapshot testing, here is reading material on automated testing in general.

- McConnell, S. (2004). *Code Complete*. Microsoft Press. (**pp. 499-533**)

- Boswell, D., & Foucher, T. (2011). *The Art of Readable Code*. O'Reilly Media, Inc. (**pp. 149-162**)

- Riccomini, C., & Ryaboy D. (2021). *The Missing Readme*. No Starch Press. (**pp. 89-108**)

- Martin, R. C. (2009). *Clean Code*. Pearson Education. (**pp. 121-133**)

- Fowler, M. (2018). *Refactoring.* Addison-Wesley Professional. (**pp. 85-100**)

- Beck, K. (2003). *Test-Driven Development*. Addison-Wesley Professional.

## Additional resources 

For a comprehensive collection of packages for unit testing in R, see [this](https://indrajeetpatil.github.io/awesome-r-pkgtools/#unit-testing) page.

# Find me at...

{{< fa brands twitter >}} [Twitter](http://twitter.com/patilindrajeets)

{{< fa brands linkedin >}} [LikedIn](https://www.linkedin.com/in/indrajeet-patil-397865174/)

{{< fa brands github >}} [GitHub](http://github.com/IndrajeetPatil)

{{< fa solid link >}} [Website](https://sites.google.com/site/indrajeetspatilmorality/)

{{< fa solid envelope >}} [E-mail](mailto:patilindrajeet.science@gmail.com)

# Thank You 

And Happy Snapshotting! 😊
